2017-06-07 03:28:29  /entrypoint.sh: static: does not generate config
HEALTH_WARN too few PGs per OSD (20 < min 30)
2017-06-07 03:28:30  /entrypoint.sh: static: does not generate the admin key, so we can not get it.
2017-06-07 03:28:30  /entrypoint.sh: static: make it available with the help of your configuration management system.
2017-06-07 03:28:30  /entrypoint.sh: static: ceph-ansible is a good candidate to deploy a containerized version of Ceph.
2017-06-07 03:28:30  /entrypoint.sh: static: ceph-ansible will help you fetching the keys and push them on the right nodes.
2017-06-07 03:28:30  /entrypoint.sh: static: if you're interested, please visit: https://github.com/ceph/ceph-ansible
2017-06-07 03:28:30  /entrypoint.sh: SUCCESS
2017-06-07 03:28:30.448032 7f71ab8ce300  0 set uid:gid to 64045:64045 (ceph:ceph)
2017-06-07 03:28:30.448074 7f71ab8ce300  0 ceph version 9.0.0-18993-gf916801 (f91680128ef51a85d28019c9afd29109bd178b1c), process ceph-mds, pid 109
starting mds.issdm-18 at -
2017-06-07 03:28:30.450111 7f71ab8ce300  0 pidfile_write: ignore empty --pid-file
2017-06-07 03:29:09.091330 7f71a42cb700  0 mds.1.cache creating system inode with ino:101
2017-06-07 03:29:09.091459 7f71a42cb700  0 mds.1.cache creating system inode with ino:60a
2017-06-07 03:29:09.091687 7f71a42cb700  0 mds.1.cache creating system inode with ino:60b
2017-06-07 03:29:09.091782 7f71a42cb700  0 mds.1.cache creating system inode with ino:60c
2017-06-07 03:29:09.092055 7f71a42cb700  0 mds.1.cache creating system inode with ino:60d
2017-06-07 03:29:09.092170 7f71a42cb700  0 mds.1.cache creating system inode with ino:60e
2017-06-07 03:29:09.092282 7f71a42cb700  0 mds.1.cache creating system inode with ino:60f
2017-06-07 03:29:09.092389 7f71a42cb700  0 mds.1.cache creating system inode with ino:610
2017-06-07 03:29:09.092480 7f71a42cb700  0 mds.1.cache creating system inode with ino:611
2017-06-07 03:29:09.092570 7f71a42cb700  0 mds.1.cache creating system inode with ino:612
2017-06-07 03:29:09.092693 7f71a42cb700  0 mds.1.cache creating system inode with ino:613
2017-06-07 03:29:13.899970 7f71a724e700  0 -- 192.168.140.230:6800/1702951379 >> 192.168.140.189:6800/2189086503 conn(0x7f71b4c9e000 :6800 s=STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH pgs=0 cs=0 l=0).handle_connect_msg accept connect_seq 0 vs existing csq=0 existing_state=STATE_CONNECTING_WAIT_CONNECT_REPLY
2017-06-07 03:29:17.736015 7f71a624c700  0 -- 192.168.140.230:6800/1702951379 >> 192.168.140.243:6800/980384507 conn(0x7f71b4c71000 :6800 s=STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH pgs=0 cs=0 l=0).handle_connect_msg accept connect_seq 0 vs existing csq=0 existing_state=STATE_CONNECTING_WAIT_CONNECT_REPLY
2017-06-07 03:29:17.736119 7f71a624c700  0 -- 192.168.140.230:6800/1702951379 >> 192.168.140.243:6800/980384507 conn(0x7f71b4c71000 :6800 s=STATE_ACCEPTING_WAIT_CONNECT_MSG pgs=0 cs=0 l=0).fault with nothing to send and in the half  accept state just closed
2017-06-07 03:44:17.737856 7f71a624c700  0 -- 192.168.140.230:6800/1702951379 >> 192.168.140.243:6800/980384507 conn(0x7f71b4ca2800 :-1 s=STATE_OPEN pgs=8 cs=1 l=0).fault with nothing to send, going to standby
