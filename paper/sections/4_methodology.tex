\section{Namespace Schemas}

\begin{figure*}[tbh]
  \centering
  \begin{subfigure}[b]{.3\linewidth}
    \centering
    For \(n\) processes on \(m\) servers:
    \begin{itemize}
      \item[] \texttt{\# of dirs =} \(m \times \texttt{mkdir()}\)
      \item[] \texttt{\# of file =} \(2 \times n \times m\)
      \item[] \texttt{\# of file per dir =} \(n/m\)
    \end{itemize}
    \caption{Function for PLFS} \label{fig:plfs}
  \end{subfigure}
  \begin{subfigure}[b]{.3\linewidth}
      \footnotesize
      \begin{minted}[xleftmargin=1em]{lua}
local box require 'box2d'
o = {}; i = 1   -- o: object list
for _x=x,x+size do for _y=y,y+size do 
  if temperature>30 then
    box0=box.nsplit(0,2,h,x,y,z,size)
    box1=box.nsplit(1,2,h,x,y,z,size)
    o[i]=box0(); i=i+1
    o[i]=box1(); i=i+1
  else o[i] = _x.._y..z.."_0"; i=i+1 end
end end
return o
      \end{minted}
      \caption{Code for SIRIUS} \label{fig:sirius}
  \end{subfigure}
  \begin{subfigure}[b]{.3\linewidth}
\noindent\texttt{pointer\_schema}: \texttt{(o0, o2, o9)}

\noindent\texttt{code\_schema}:

      \centering
      \footnotesize
      \begin{minted}[xleftmargin=1em]{c++}
void recurseBranch(TObjArray *o) {
  TIter i(o); 
  for(TBranch *b=i.Next(); i.Next()!=0; b=i.Next()) {
    processBranch(b);
    recurseBranch(b->GetListOfBranches());
  }
}
      \end{minted}
      \caption{Pointer/Code for HEP} \label{fig:hep}
  \end{subfigure}
\caption{Namespace schemas that generate subtrees for 3 motivating examples.\label{fig:use-cases}}
\end{figure*}

%char *tn = getTreeName().c_str();
%TTree* t = (TTree*) root->Get(tn);
%TIter i(t->GetListOfBranches());
%for(TBranch *b = i.next();
%    i.Next() != 0;
%    b = (TBranch*) i.Next())
%  recurseBranch(b->GetListOfBranches());

For three domain-specific applications and use-cases, we have identified
different scalability bottlenecks:

\begin{enumerate}
  \item namespaces with many requests
  \item namespaces managed by remote servers
  \item namespaces that are too large
\end{enumerate}

We propose namespace schemas to support lazy generation of metadata at
different nodes in the storage system.  A centralized, globally consistent
metadata service (either a single metadata server or a cluster of active-active
metadata servers) provides clients with (1) the root of the subtree of interest
and (2) a namespace schema. Using the namespace schema, clients and servers do
not need to materialize the namespace up front, thus avoiding the costs of
making the tree with metadata RPCs.

The namespace schema is stored in the directory inode of the root of the
subtree that the client cares about. We use the ``file type" interface from the
Malacology~\cite{sevilla:eurosys17-malacology} project to facilitate this
domain-specific functionality. This is similar to push-down predicates in
databases, where the application is providing domain-specific knowledge that
the storage system knows how to leverage.  We have defined three types of
namespace schemas: a formula, a code, and a pointer.

\subsection{Formula Schema} 

For this schema, a formula that generates the file system namespace is stored
in the inode. This formula takes domain-specific information as input and
produces a list of files and directories.

For example, because PLFS deterministically creates files and directories based
on the number of clients, Tintenfisch can use a formula schema like the one in
Figure~\ref{fig:plfs}. Specifically, the function takes as input the number of
processes and hosts in the cluster and outputs the number of directories, the
number of files, and the number of files per directory. For the example
namespace drawn in Figure~\ref{fig:tree_plfs} the input to the formula is 3
hosts each with 1 process and the output is 3 directories and 6 files, with 2
files per directory.

%For \(n\) processes on \(m\) servers:
%\begin{itemize}
%  \item[] \texttt{\# of dirs =} \(m \times \texttt{mkdir()}\)
%  \item[] \texttt{\# of file =} \(2n \times \texttt{create()} [+ 2n \times \texttt{lookup()}]\)
%  \item[] \texttt{\# of file per dir =} \(n/m\)
%\end{itemize}

\subsection{Code Schema}

Sometimes the namespace schema logic is too complex to store as a single
function or requires external libraries are needed to interpret metadata. For
example, the SIRIUS use-case constructs the file system namespace using Lua
(for sandboxing purposes) and complicated partitioning logic. To facilitate the
partitioning function as is, we provide a code schema that allows users to
write programs that generate the namespace.

A sample code schema for the SIRIUS project is shown in
Figure~\ref{fig:sirius}.  Although the partitioning function itself is not
constructed by iterating through the bounding box coordinates and checking if a
threshold temperature is eclipsed. If it is, then extra objects are generated
using the \texttt{box2d} Lua package.  realistic, it shows how code schemas can
accomodate domain-specific data layout policies that are complex and/or require
external libraries.  An object list is

\subsection{Pointer Schema} 

Sometimes there is no formal specification for the namespace. For example, the
ROOT framework uses self-describing files so headers and metadata need to be
read for each ROOT file. In these scenarios, a code stored in the inode is
insufficient for generating the file system namespace because all necessary
metadata is in large objects. Pointer schemas reference a large object in
scalable storage and support function shipping to read parts of the object.

For example, a code schema containing library code for the ROOT framework
\emph{and} a pointer schema for referencing the input to the code can be used
to describe a ROOT file system namespace. The pointer schema would point to
objects in the object store that contain the necessary metadata for
constructing the file system namespace. This is shown in Figure~\ref{fig:hep};
clients requesting Branches would follow the pointer schema to the objects
containing metadata and would read them using the code schema. This avoids
storing all metadata in inodes, which is a concern for distributed file systems
like CephFS.

\subsection*{A Fresh, Unorthodox, Unexpected, Controversial, and Counterintuitive Idea}

Global file systems can be scalable if programmed correctly. For example, the
following notions are out-dated:

\begin{itemize}

  \item robust so they are fast... but we show that today's apps are so large
  that we need to specialized storage systems

  \item general because they have been around for so long... but we show that
  most apps don't need fs metadata

  \item subject to data IO performance... but we show that metadata is slow

\end{itemize}


%In this section, we show how clients and metadata servers communicate using the
%Pattern PLFS language and present our
%storage system that adapts to the wokload
%(Section~\ref{sec:adapting-to-the-workload-with-cudele})).  Other destructive
%solutions include changing the storage system and altering the application.
%
%\subsection{Adapting to the Workload with Cudele}
%\label{sec:adapting-to-the-workload-with-cudele}
%
%\begin{figure}[tb]
%\centering
%  \includegraphics[width=90mm]{figures/arch.png} 
%  \caption{System XX lets clients optimize performance by telling the storage
%  system about the workload. Clients can specify a Structured Namespace (blue
%  subtrees and Section~\ref{sec:structured-namespaces}) or by merging file system
%  metadata from an Unstructured Namespace (red subtree and
%  Section~\ref{sec:unstructured-namespaces}).}\label{fig:arch}
%\end{figure}
%
%% What is Cudele
%Cudele is a file system with programmable consistency and durability. Clients
%use an API to decouple existing subtrees from the global namespace; metadata
%operations from the other clients targeted at the decoupled subtree can be
%programmed to be blocked or marked as overwritable. With the decoupled subtree
%in hand, the client can do metadata operations locally. Upon completion, the
%client can merge the subtree back into the global namespace. 
%
%% Why Cudele is a good fit for implied namespaces
%Cudele has the mechanisms for understanding the file system metadata language
%and adapting to the workload.  Figure~\ref{fig:arch} shows how clients decouple
%the namespace with the Cudele API, specifying how many extra inodes they want
%and the structure for the namespace they intend to create. The metadata server
%and client both know about the metadata in the blue subtree, requiring no RPCs,
%and if the client creates more metadata (red subtree), it can merge it back
%into the global namespace.  This model lets users enjoy the simplicity of
%global namespaces and the high performance of node-local operations.  We extend
%the API to support the declaration of structured namespaces and leverage the
%existing API to merge unstructured namespaces. 
%
%\subsubsection{Structured Namespaces}
%\label{sec:structured-namespaces}
%
%% What is a structured namespace
%A structured namespace is created according to a pattern. If both the client
%and metadata server knows the pattern, they can create the metadata
%independently. This has two benefits: (1) it reduces RPCs which improves
%performance and reduces network traffic and (2) it allows the client and server
%to operate in parallel.  The patterns that Cudele understands are shown in
%Listing~\ref{src:example} and the programmable interfaces are shown below.
%There are two parameters for unstructured namespaces: \texttt{pattern} and
%\texttt{trigger}. 
%
%\subsubsection{Trigger: Start Namespace Construction}
%
%% How does trigger work and why do we neet it
%\texttt{trigger} specifies when to start the namespace construction on the
%metadata server.  The metadata reconstruction can be asynchronous and saving
%this resource intense process for later can have better performance. To
%facilitate the exploration of different trigger policies, we make the value for
%the \texttt{trigger} parameter programmable.  Administrators inject Lua code
%that specifies or calculates thresholds for when to start namespace
%construction. Although we make this programmable, we do not make any
%conclusions about the best trigger time and leave the exploration of this space
%as future work.
%
%% example
%In Listing~\ref{src:example}, the trigger is:
%\begin{listing}
%\begin{minted}[frame=single,
%               framesep=2mm,
%               xleftmargin=10pt,
%               tabsize=2]{lua}
%{
%  if MDSs[whoami]["cpu"] > 30
%}
%\end{minted}
%\label{src:thresh}
%\end{listing}
%
%which means that construction of the namespace will start if current MDS
%(\texttt{whoami}) has a CPU utilization (\texttt{``cpu"}) above 30\%.
%
%% Drawbacks: consistency
%Triggering construction asynchronously can improve performance because the
%process can be deferred until the system has less load. However, this
%performance gain comes at the cost of consistency. Even if the construction is
%triggered immediately, the metadata is eventually consistent; other clients see
%outdated metadata because the namespace is sitting on the client. Delaying the
%trigger improves the liklihood that system finds a window of low load but also
%increases the latency of other clients.\\
%
%\noindent\textbf{Implementation}: we re-use the polling and embedded Lua
%virtual machine in Mantle~\cite{sevilla:sc15-mantle} to implement the trigger
%interface. By default, every 10 seconds the metadata server checks if the
%condition for triggering is satisfied by executing the Lua code. Mantle has
%variables exposed for administrators to explore load balancing policies; just
%like this work, some of these policies need to identify overloaded metadata
%servers so we re-use all those variables.  Some of the more useful variables
%include:
%
%\begin{itemize}
%  \item Memory Usage
%  \item CPU Utilization
%  \item Request Rate
%  \item Queue Depth
%  \item Server Tags: whoami, i
%\end{itemize}
%
%\subsubsection{Pattern: Express Namespace}
%\label{sec:pattern-express-namespace}
%
%% How does pattern work and why do we need it
%\texttt{pattern} describes the metadata layout of the Structured Namespaces. It
%is the same language used in~\cite{he:hpdc13-plfs-patterns}. When the metadata
%server starts a namespace construction, it creates all the file system metadata
%generated by this formula. As a refresher, the pattern in Listing~\ref{src:example}:
%
%  \[[i, (d[0], d[1], ...)^r]\]
%
%means that there are \(r\) entries in the PLFS index file, where the first
%entry has a physical offset of \(i\) and lengths of \(d\), where the pattern in
%\(d\) repeats. \\
%
%% WTF -- this doesn't give file system metadata! ARGGGGG is it file creations
%% or index files shit?
%
%% Drawbacks
%
%\noindent\textbf{Implementation}: Another big fat TODO.
%
%\begin{listing}
%\begin{minted}[frame=single,
%               framesep=2mm,
%               xleftmargin=10pt,
%               tabsize=2]{js}
%{
%  <!-- Structured Namespace Pattern !-->
%  "S_pattern": "[i, (d[0], d[1], ...)^r]",
%  
%  <!-- Structured Namespace Trigger !-->
%  "S_trigger": "if MDSs[whoami]["cpu"] > 30",
%  
%  <!-- Untructured Namespace Allocated Inos !-->
%  "US_alloci": "1000",
%}
%\end{minted}
%\caption{Using the Cudele API to express metadata structure, which is
%understood by both the server and client.}
%\label{src:example}
%\end{listing}
%
%\subsubsection{Unstructured Namespaces}
%\label{sec:unstructured-namespaces}
%
%\subsubsection{Migrating Metadata Construction}
%\label{sec:migrating-metadata-construction}
%
